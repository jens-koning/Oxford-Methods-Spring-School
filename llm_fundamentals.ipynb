{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSCgHXSB1xfaZCiShlio4G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antndlcrx/Oxford-Methods-Spring-School/blob/main/llm_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://cdn.githubraw.com/antndlcrx/oss_2024/main/images/dpir_oss.png?raw=true:,  width=70\" alt=\"My Image\" width=500>\n",
        "\n",
        "# **LLM Fundamentals**"
      ],
      "metadata": {
        "id": "-GlQculfR2bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language models have demonstrated remarkable capabilities in generating and understanding human-like language, profoundly impacting modern NLP applications. Their success is primarily driven by two factors:\n",
        "\n",
        "- **Architecture**: Modern language models use **transformer** neural network architecture (with attention mechanisms at its core), which enables (sub)words or tokens to dynamically adjust their meanings based on surrounding context. This architecture **allows models to capture nuanced linguistic relationships and adapt flexibly to different contexts**.\n",
        "\n",
        "- **Scalability**: These models excel because they effectively scale to billions of parameters and learn from massive datasets. **Large datasets expose models to diverse linguistic patterns**, enriching their representations, while increasing the **number of parameters enables the capture of subtle, context-dependent language nuances**."
      ],
      "metadata": {
        "id": "0tDc9vibwVK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üóìÔ∏èOutlook:\n",
        "\n",
        "This session covers:\n",
        "\n",
        "- **Language Modelling** (what is a model, what is language, how to build a model of language)\n",
        "- **Tokenisation** (How to process text)\n",
        "- **Architecture** (How to build a model)\n",
        "- **Inference** (given the model, how do we generate text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxCBVDn1R73-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Data for this Session\n",
        "\n",
        "import requests\n",
        "\n",
        "GUTENBERG_URLS = {\n",
        "    \"pride_and_prejudice.txt\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"sense_and_sensibility.txt\": \"https://www.gutenberg.org/files/161/161-0.txt\",\n",
        "    \"mansfield_park.txt\": \"https://www.gutenberg.org/files/141/141-0.txt\"\n",
        "}\n",
        "\n",
        "DELIMITER = \"\\n<|endoftext|>\\n\\n\"\n",
        "COMBINED_FILENAME = \"austen_combined.txt\"\n",
        "\n",
        "def download_file(filename, url):\n",
        "    \"\"\"Download a file and save it locally.\"\"\"\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "def combine_files(file_list, output_file, delimiter):\n",
        "    \"\"\"Combine a list of files into one, separated by a delimiter.\"\"\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for fname in file_list:\n",
        "            with open(fname, \"r\", encoding=\"utf-8\") as infile:\n",
        "                text = infile.read().strip()\n",
        "                outfile.write(text + delimiter)\n",
        "\n",
        "\n",
        "for fname, url in GUTENBERG_URLS.items():\n",
        "    download_file(fname, url)\n",
        "combine_files(GUTENBERG_URLS.keys(), COMBINED_FILENAME, DELIMITER)\n",
        "\n",
        "\n",
        "# with open(\"austen_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#     raw_text = f.read()\n",
        "\n",
        "# len(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmg3RY6Pvbm_",
        "outputId": "5e3f04ab-9072-4d60-cfee-7d3c1b954df8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pride_and_prejudice.txt...\n",
            "Downloading sense_and_sensibility.txt...\n",
            "Downloading mansfield_park.txt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1**.&nbsp; What is a Language Model?"
      ],
      "metadata": {
        "id": "JHMrcCLO4A-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÆ **Language Modeling Objective**\n",
        "\n",
        "Language modeling is a fundamental task in natural language processing (NLP), where the goal is to predict the next word in a sequence based on the preceding words. In other words, a language model aims to estimate the probability distribution of word sequences in a given language. Training a language model involves maximizing the likelihood that the model assigns to actual word sequences observed in the training data.\n",
        "\n",
        "```perl\n",
        "Predict next word based on previous context\n",
        "\n",
        "Given a word sequence:\n",
        "      w‚ÇÅ ‚Üí w‚ÇÇ ‚Üí w‚ÇÉ ‚Üí ... ‚Üí w‚Çô‚Çã‚ÇÅ ‚Üí w‚Çô\n",
        "       ‚îÇ    ‚îÇ    ‚îÇ            ‚îÇ\n",
        "       ‚ñº    ‚ñº    ‚ñº            ‚ñº\n",
        "Predict:   Predict:   Predict:          Predict:\n",
        "   w‚ÇÇ         w‚ÇÉ         w‚ÇÑ                 w‚Çô\n",
        "given:      given:      given:            given:\n",
        "  w‚ÇÅ       w‚ÇÅ,w‚ÇÇ      w‚ÇÅ,w‚ÇÇ,w‚ÇÉ        w‚ÇÅ,w‚ÇÇ,...,w‚Çô‚Çã‚ÇÅ\n",
        "```\n",
        "\n",
        "**Mathematical Description**\n",
        "\n",
        "Formally, given a sequence of words $(w_1, w_2, \\dots, w_N)$, the language modeling objective is to maximize the joint probability of the entire sequence. Using the chain rule of probability, this joint probability can be decomposed into a product of conditional probabilities:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_N) = P(w_1) \\cdot P(w_2 \\mid w_1) \\cdot P(w_3 \\mid w_1, w_2) \\dots P(w_N \\mid w_1, w_2, \\dots, w_{N-1})\n",
        "$$\n",
        "\n",
        "In practice, a language model predicts each word $w_n$ based solely on the preceding words, thus learning conditional probabilities:\n",
        "\n",
        "$$\n",
        "P(w_n \\mid w_1, w_2, \\dots, w_{n-1})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üìâ**Finding a Good Model: Negative Log Likelihood**\n",
        "To effectively train language models, the objective is typically framed as minimizing the **negative log-likelihood** of these probabilities over the training corpus:\n",
        "\n",
        "$$\n",
        "\\text{Minimize: } -\\log P(w_1, w_2, \\dots, w_N) = -\\sum_{n=1}^{N} \\log P(w_n \\mid w_1, w_2, \\dots, w_{n-1})\n",
        "$$\n",
        "\n",
        "- **Likelihood**: how probable is our data given the current model. We want our language model to assign high probabilities to the actual words seen during training.\n",
        "- **Why Log?**: Probabilities multiply quickly, becoming very small numbers. **Taking a logarithm transforms these multiplications into sums, which are numerically stable and computationally convenient**. Instead of multiplying many small probabilities, we add their logarithms.\n",
        "- **Why Negative?**: Our goal is to maximize likelihood, but mathematically it is more convenient to frame optimization problems as minimization. Thus, we minimize the negative of the log likelihood. **Minimizing the negative log likelihood is equivalent to maximizing the likelihood**.\n",
        "\n",
        "This negative log-likelihood measure is computationally convenient and helps the model learn meaningful linguistic patterns by penalizing low-probability predictions.\n",
        "\n",
        "```perl\n",
        "Minimize negative log-likelihood:\n",
        "  \n",
        "‚àí log [P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)]\n",
        "           ‚îÇ\n",
        "           ‚ñº\n",
        "= ‚àí [log P(w‚ÇÅ) + log P(w‚ÇÇ|w‚ÇÅ) + log P(w‚ÇÉ|w‚ÇÅ,w‚ÇÇ) + ... + log P(w‚Çô|w‚ÇÅ,...,w‚Çô‚Çã‚ÇÅ)]\n",
        "```\n",
        "---\n",
        "\n",
        "### ü§î**Evaluation: Perplexity**\n",
        "**Perplexity** is a measure used to evaluate how well a language model predicts unseen text. Intuitively, it answers the question: \"How many equally likely words is my model choosing between?\"\n",
        "\n",
        "Formally, perplexity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity} = e^{-\\frac{1}{N}\\sum_{n=1}^{N}\\log P(w_n|w_1,\\dots,w_{n-1})}\n",
        "$$\n",
        "\n",
        "We initially computed the **average negative log likelihood** (or cross-entropy). Taking the exponential **transforms the log-scale back to a normal scale**, giving us a measure that's intuitive to interpret as an effective \"branching factor.\"\n",
        "\n",
        "- **Lower perplexity** ‚Üí the model is confident and accurate, fewer \"choices\" per step.\n",
        "- **Higher perplexity** ‚Üí the model is uncertain, predicting many possible next words.\n",
        "\n",
        "üß† Quick Intuitive Example:\n",
        "- A perplexity of 1000 means the model is roughly guessing among 1000 possible words for every prediction ‚Äî a poor model.\n",
        "\n",
        "- A perplexity of 10 means the model consistently narrows down to about 10 possible words ‚Äî a much better model.\n",
        "\n",
        "> üìñ For more:\n",
        "- [Language Modelling NLP Course for You](https://lena-voita.github.io/nlp_course/language_modeling.html)\n",
        "- [Perplexity of fixed-length models\n",
        "by ü§ó](https://huggingface.co/docs/transformers/en/perplexity)\n"
      ],
      "metadata": {
        "id": "CWM1ROXn4FWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2**.&nbsp; üçì **Tokenisation**\n"
      ],
      "metadata": {
        "id": "YrL78bFpTqN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Motivation**\n",
        "\n",
        "**Tokenization** is the critical step of **converting human-readable text into numerical representations that language models can process**.\n",
        "\n",
        "After defining language modeling as predicting words from context, the natural next question is: **how exactly do we represent words numerically**? This is precisely the role of tokenization, bridging the complexity of natural language to structured numeric inputs suitable for neural networks.\n",
        "\n",
        "Tokenization serves several key purposes:\n",
        "\n",
        "1. **Reduction of Vocabulary Size**:\n",
        "Natural languages are vast, filled with misspellings, colloquialisms, technical jargon, and new words constantly emerging. Tokenization condenses this enormous diversity into a fixed, manageable vocabulary, making computations feasible.\n",
        "\n",
        "2. **Efficient Computation**:\n",
        "Transforming words (or subwords) into numeric indices lets models efficiently perform mathematical operations required by neural networks.\n",
        "\n",
        "3. **Meaningful Representation**:\n",
        "Tokenization methods, especially subword approaches (like Byte-Pair Encoding or WordPiece), effectively handle semantic similarities. They break words down into meaningful parts, allowing models to generalize across related terms or word forms, even if the model hasn‚Äôt explicitly encountered them during training."
      ],
      "metadata": {
        "id": "K59EBJm_lpYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Toy Example**"
      ],
      "metadata": {
        "id": "QJsFQgn7WcP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "with open(\"austen_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "len(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz1D2qvgANBO",
        "outputId": "a7cae8df-4c8a-46a6-fcf6-1c9056541b7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2302675"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load library\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab') # pre-trained models and rules for splitting text into sentences and words, handling punctuation, abbreviations, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJRbsi2ZUSXJ",
        "outputId": "0e6f0950-ba96-49fa-a29c-a37f0c0db7e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean Up the Raw Text\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_gutenberg_text(text):\n",
        "    \"\"\"\n",
        "    Cleans Gutenberg text by removing header, footer, and metadata.\n",
        "    \"\"\"\n",
        "    # Remove header\n",
        "    text = re.split(r\"\\*\\*\\* START OF (THE|THIS) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", text, flags=re.IGNORECASE)[-1]\n",
        "\n",
        "    # Remove footer\n",
        "    text = re.split(r\"\\*\\*\\* END OF (THE|THIS) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", text, flags=re.IGNORECASE)[0]\n",
        "\n",
        "    # Remove illustration tags and bracketed contents\n",
        "    text = re.sub(r\"\\[Illustration.*?\\]\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Remove \"Contents\" and chapter listings (ToC)\n",
        "    text = re.split(r\"Contents\\n\\n\", text, flags=re.IGNORECASE)\n",
        "    if len(text) > 1:\n",
        "        text = re.split(r\"\\n{2,}(CHAPTER\\s+I\\b)\", text[1], flags=re.IGNORECASE)\n",
        "        text = \"\".join(text[-2:]) if len(text) >= 2 else text[-1]\n",
        "    else:\n",
        "        text = text[0]\n",
        "\n",
        "    # Remove excessive newlines and whitespace\n",
        "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_gutenberg_text(raw_text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1iPI44wHDyXk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build a Toy Tokenizer\n",
        "# tokenize text\n",
        "\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "token_set = sorted(set(tokens))\n",
        "\n",
        "print(len(tokens), len(token_set))\n",
        "\n",
        "# create vocab\n",
        "vocab = {t:i for i,t in enumerate(token_set)}\n",
        "vocab[\"a\"]\n",
        "\n",
        "class SimpleTokenizer():\n",
        "    def __init__(self, tokenizer_train_text):\n",
        "        self.token_set = sorted(set(tokenizer_train_text.split()))\n",
        "        self.vocab = {t:i for i,t in enumerate(self.token_set)} # text to token ids mapping\n",
        "        self.vocab[\"<unk>\"] = len(self.vocab) + 1 # add unk\n",
        "        self.inverse_vocab = {i:t for t,i in self.vocab.items()} # token ids to text mapping\n",
        "\n",
        "    def encode(self, x: str):\n",
        "        worldlist = x.split()\n",
        "        input_ids = [self.vocab.get(word, self.vocab[\"<unk>\"]) for word in worldlist]\n",
        "        return input_ids\n",
        "\n",
        "    def decode(self, x: list[int]):\n",
        "        outputs = [self.inverse_vocab.get(token_id) for token_id in x]\n",
        "        return \" \".join(outputs)\n",
        "\n",
        "tokenizer = SimpleTokenizer(cleaned_text)\n",
        "\n",
        "test = tokenizer.encode(cleaned_text[:995])\n",
        "test_decoded = tokenizer.decode(test)\n",
        "\n",
        "test = tokenizer.encode(\"I like walking my dog in the evenings in the University park where sunsets are just so beautiful.\")\n",
        "test_decoded = tokenizer.decode(test)\n",
        "print(test_decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUP5c_4DDm_i",
        "outputId": "084e4553-80d5-4a09-9f06-cf7773955887"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "186293 9508\n",
            "I like walking my <unk> in the evenings in the <unk> park where <unk> are just so <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîó **Byte-Pair Encoding (BPE)**\n",
        "\n",
        "A common challenge in language modeling is dealing with words that weren't present in the training data. **Byte-Pair Encoding (BPE)**, introduced by [Sennrich et al. (2015)](https://arxiv.org/abs/1508.07909), elegantly solves this by breaking words into smaller, meaningful subword units. The key idea is simple yet powerful: it iteratively merges the most frequent pairs of bytes or characters in the training corpus to build a flexible vocabulary. By doing so, BPE allows models to handle unseen or rare words effectively, dramatically improving their generalization.\n",
        "\n",
        " > üìñ For an in-depth exploration of BPE, check out the [Hugging Face NLP Course (Chapter 6)](https://huggingface.co/learn/nlp-course/en/chapter6/5), or watch [Andrej Karpathy's \"Let's Build a GPT Tokenizer\" video](https://www.youtube.com/watch?v=zduSFxRajkE).\n",
        "\n",
        "\n",
        "> üìö Several libraries implement BPE:\n",
        "\n",
        "- [**Tiktoken** by OpenAI](https://github.com/openai/tiktoken)\n",
        "- [**SentencePiece** by Google](https://github.com/google/sentencepiece)\n",
        "\n",
        "üõ†Ô∏è Try exploring how tokenizers process text directly in the [Tiktokenizer app](https://tiktokenizer.vercel.app/).\n",
        "\n"
      ],
      "metadata": {
        "id": "2P17zLxshnVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxtsqqdgBAS",
        "outputId": "1863608c-4550-4fc6-ed88-c5e5641d7ce6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "d5R8ZHNTgDUH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.n_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxI6pvjOMaVn",
        "outputId": "7f69ad6e-f5c7-467f-9875-cab5889d6024"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.decode([0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9N1NjixH_WEa",
        "outputId": "316f8fda-b98c-4f13-e313-4f4d7b607885"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = bpe_tokenizer.encode(\"I like walking my dog in the evenings in the University park where sunsets are just so beautiful.\")\n",
        "print(tokens)\n",
        "bpe_tokenizer.decode(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "JihDogMOhJVM",
        "outputId": "ba764cac-3cfd-406f-839d-d34da77901de"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 588, 6155, 616, 3290, 287, 262, 37119, 287, 262, 2059, 3952, 810, 4252, 28709, 389, 655, 523, 4950, 13]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I like walking my dog in the evenings in the University park where sunsets are just so beautiful.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß**Torch DataLoader and DataSet**\n",
        "\n",
        "In PyTorch, a `Dataset` provides an organized way of accessing and managing your data, while a `DataLoader` handles batching, shuffling, and efficiently loading data during training. Specifically, a `Dataset` defines how individual data samples (inputs and labels) are accessed, while a `DataLoader` wraps around it to deliver batches seamlessly to your model. Together, they simplify data management, enhance training speed, and help ensure reproducible and robust training pipelines."
      ],
      "metadata": {
        "id": "UMKnoa91pOI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use these tools to create a flow of input-target pairs of tokens to train a language model.\n",
        "\n",
        "Suppose we have a sequence of tokens:\n",
        "\n",
        "```ini\n",
        "token_ids = [t‚ÇÄ, t‚ÇÅ, t‚ÇÇ, t‚ÇÉ, ..., t‚Çô‚Çã‚ÇÇ, t‚Çô‚Çã‚ÇÅ, t‚Çô]\n",
        "```\n",
        "\n",
        "We construct training examples by defining a context lenghth (for example `context_len=4`) and a sliding window (`stride`=2) as follows:\n",
        "\n",
        "```less\n",
        "Iteration 1:\n",
        "    Input (X):   [t‚ÇÄ,   t‚ÇÅ,   t‚ÇÇ,   t‚ÇÉ]\n",
        "    Target (Y):  [t‚ÇÅ,   t‚ÇÇ,   t‚ÇÉ,   t‚ÇÑ]\n",
        "\n",
        "Iteration 2 (stride forward by 2):\n",
        "    Input (X):   [t‚ÇÇ,   t‚ÇÉ,   t‚ÇÑ,   t‚ÇÖ]\n",
        "    Target (Y):  [t‚ÇÉ,   t‚ÇÑ,   t‚ÇÖ,   t‚ÇÜ]\n",
        "\n",
        "Iteration 3:\n",
        "    Input (X):   [t‚ÇÑ,   t‚ÇÖ,   t‚ÇÜ,   t‚Çá]\n",
        "    Target (Y):  [t‚ÇÖ,   t‚ÇÜ,   t‚Çá,   t‚Çà]\n",
        "\n",
        "...\n",
        "```\n",
        "\n",
        "until no full sequences remain.\n",
        "\n",
        "The **context window** is the number of tokens an LLM considers simultaneously when predicting the next token. A **longer context window gives the model more information and improves its ability to capture meaningful relationships**, but at the cost of increased computational requirements.\n",
        "\n",
        "The **stride determines how much the context window moves forward between each training example**. A smaller stride creates more overlapping examples, increasing the amount of training data but also introducing redundancy. A larger stride reduces overlap and speeds up data preparation but can reduce the diversity of training examples. Choosing these parameters involves balancing model performance, computational efficiency, and the richness of training data."
      ],
      "metadata": {
        "id": "_haAkNHvOrcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "RBy96RwXpS8G"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bpe_tokenizer.encode(cleaned_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-DlCVdGWkaH",
        "outputId": "0e38d44d-71d0-42a6-846f-e065158879de"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223694"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.randn((2, 3))\n",
        "x2 = torch.randn(3, 2)\n",
        "print(x1, \"\\n\"*2, x2, \"\\n\"*2, (x1 @ x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb9Y9YEXzmic",
        "outputId": "78f7df5f-09b9-4397-a8f1-c0122d7939d8"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0668,  0.0614,  0.4891],\n",
            "        [ 1.1619, -0.5594, -0.4369]]) \n",
            "\n",
            " tensor([[-0.8001,  0.9704],\n",
            "        [ 1.6100,  1.7920],\n",
            "        [ 0.2744,  0.6635]]) \n",
            "\n",
            " tensor([[ 0.1796,  0.4993],\n",
            "        [-1.9501, -0.1649]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Dataset and DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer, context_len, stride):\n",
        "        super().__init__()\n",
        "        self.Y = []\n",
        "        self.X = []\n",
        "\n",
        "        input_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "        for i in range(0, len(input_ids) - context_len, stride):\n",
        "            xids = input_ids[i: i + context_len]\n",
        "            yids = input_ids[i + 1: i + 1 + context_len]\n",
        "\n",
        "            self.X.append(torch.tensor(xids))\n",
        "            self.Y.append(torch.tensor(yids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "ds = CustomDataset(cleaned_text, bpe_tokenizer, 64, 64)\n",
        "\n",
        "\n",
        "### Train Val Split ###\n",
        "dataset_size = len(ds)\n",
        "train_size = int(0.9 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "print(f\"Train size: {train_size}; Val size: {val_size}\")\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds = random_split(ds, [train_size, val_size], generator=generator)\n",
        "\n",
        "### Create DataLoaders ###\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=True,  # shuffle for training\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=False,  # no need to shuffle for validation\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UTHZrWtN9Ey",
        "outputId": "a65d94b7-59b7-4981-bac7-d4cd110d62d5"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 3145; Val size: 350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "for i, (x,y) in enumerate(train_loader):\n",
        "    print(f'batch {i}:',\"\\n\", x, \"\\n\", y)\n",
        "    print(f'batch {i}:',\"\\n\", x.shape, \"\\n\", y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2t4LfR8OPX0",
        "outputId": "5d71e743-b525-429e-fd2b-060fd52a3ac2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0: \n",
            " tensor([[  198,   198, 22788,  ..., 32649,   507,   286],\n",
            "        [  286,  2279,   198,  ...,   286, 12921,   475],\n",
            "        [ 3860,   286,   257,  ..., 29023,   540,   284],\n",
            "        ...,\n",
            "        [  866,   355,   262,  ...,  3675,   477,   198],\n",
            "        [  976,   661,   284,  ...,   502,   922,   357],\n",
            "        [ 1122,   198, 19188,  ...,   290,  1583,    13]]) \n",
            " tensor([[  198, 22788,  5658,  ...,   507,   286,   262],\n",
            "        [ 2279,   198,  7091,  ..., 12921,   475,   198],\n",
            "        [  286,   257,   614,  ...,   540,   284,    13],\n",
            "        ...,\n",
            "        [  355,   262,   198,  ...,   477,   198,  9948],\n",
            "        [  661,   284,   804,  ...,   922,   357,  1219],\n",
            "        [  198, 19188,   307,  ...,  1583,    13, 12181]])\n",
            "batch 0: \n",
            " torch.Size([64, 64]) \n",
            " torch.Size([64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3**.&nbsp; **Building a Transformer Language Model**"
      ],
      "metadata": {
        "id": "Pt4bGZt4pUbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nearly all SOTA LLMs and their predaccessors are a version of a **transformer** architecture. Transformer is a neural net architecture that mainly relies on attention. Was first introduced by [Vaswani et al. 2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
        "\n",
        "The main idea is of the transformer is to **create a neural network that understands the meaning of each word by looking directly at every other word in the sentence at the same time**, rather than reading words one-by-one. This allows the network to better grasp context and relationships between words, significantly improving its ability to process language effectively..\n",
        "\n",
        "> üîßExplore the model via online visualisation tools:\n",
        "- [LLM visualisation](https://bbycroft.net/llm).\n",
        "- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/).\n",
        "- [Classic: Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/).\n",
        "- [Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)."
      ],
      "metadata": {
        "id": "2yTQG2aLIgSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 1**.&nbsp; **Input Embeddings**"
      ],
      "metadata": {
        "id": "tdBDZkWypYRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer is a look-up operation, which we can use to get the representation of a token by indexing it via token id.\n",
        "\n",
        "In transformer LMs, we econde both individual tokens themselves, and their positions in the sentence."
      ],
      "metadata": {
        "id": "wjsX9WF6SXnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Embedding Layer Showcase\n",
        "torch.manual_seed(42)\n",
        "\n",
        "embd_layer = torch.nn.Embedding(5, 2)\n",
        "x = torch.tensor([4, 3, 2, 1, 0])\n",
        "print(embd_layer.weight, \"\\n\",\"\\n\", embd_layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Aq009rCNaG9i",
        "outputId": "25a6dcde-0d0d-45f9-9b0b-2ce2b5d480b4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3367,  0.1288],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [-1.1229, -0.1863],\n",
            "        [ 2.2082, -0.6380],\n",
            "        [ 0.4617,  0.2674]], requires_grad=True) \n",
            " \n",
            " tensor([[ 0.4617,  0.2674],\n",
            "        [ 2.2082, -0.6380],\n",
            "        [-1.1229, -0.1863],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [ 0.3367,  0.1288]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build a Base LM\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, n_embd, n_hidden, tokenizer, device):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embd\n",
        "        self.n_hidden = n_hidden\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        self.embd = torch.nn.Embedding(self.tokenizer.n_vocab, self.n_embd)\n",
        "        self.rnn = torch.nn.RNN(self.n_embd, self.n_hidden, batch_first=True)\n",
        "        self.out = torch.nn.Linear(self.n_hidden, self.tokenizer.n_vocab)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embd(x)  # [batch, context_len, n_embd]\n",
        "        x, hidden = self.rnn(x)  # [batch, context_len, n_hidden]\n",
        "        logits = self.out(x)  # [batch, context_len, vocab_size]\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=10, lr=1e-3):\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device)\n",
        "\n",
        "                logits = self(X)\n",
        "                logits = logits.view(-1, self.tokenizer.n_vocab)\n",
        "                Y = Y.view(-1)\n",
        "                loss = loss_fn(logits, Y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            average_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            if val_loader is not None:\n",
        "                self.eval()\n",
        "                total_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device)\n",
        "                        val_logits = self(Xv)\n",
        "                        val_logits = val_logits.view(-1, self.tokenizer.n_vocab)\n",
        "                        Yv = Yv.view(-1)\n",
        "\n",
        "                        val_loss = loss_fn(val_logits, Yv)\n",
        "                        total_val_loss += val_loss.item()\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens):\n",
        "        self.eval()\n",
        "        input_ids = self.tokenizer.encode(prompt)\n",
        "        input_ids = torch.tensor(input_ids).to(self.device).unsqueeze(0) # batch dim\n",
        "\n",
        "        for i in range(max_new_tokens):\n",
        "            with torch.no_grad():\n",
        "                logits = self(input_ids) # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "            last_logit = logits[:, -1, :]\n",
        "            probs = torch.softmax(last_logit, dim=-1)\n",
        "\n",
        "            next_token = torch.multinomial(probs, num_samples=1) # sample 1 token from probs\n",
        "            input_ids = torch.cat((input_ids, next_token), dim=1) # add new token id to sequence of input_ids\n",
        "\n",
        "        generated_tokens = input_ids[0].tolist()  # remove batch dimension\n",
        "        return self.tokenizer.decode(generated_tokens)\n",
        "\n",
        "### training ###\n",
        "\n",
        "## uncomment in case CUDA out of memory\n",
        "# del model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "config = {\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\"\n",
        "}\n",
        "\n",
        "model = LanguageModel(n_embd=32, n_hidden=32, **config)\n",
        "model.fit(train_loader, val_loader, epochs=5, lr=0.01)\n",
        "\n",
        "\n",
        "### inference ###\n",
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E5WUBxIb6nT",
        "outputId": "cd4e1b6a-772e-46a7-c2b0-62e9d9c511e5"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]  Train Loss: 7.270  |  Val Loss: 6.463\n",
            "Epoch [2/5]  Train Loss: 6.096  |  Val Loss: 6.107\n",
            "Epoch [3/5]  Train Loss: 5.754  |  Val Loss: 5.790\n",
            "Epoch [4/5]  Train Loss: 5.457  |  Val Loss: 5.587\n",
            "Epoch [5/5]  Train Loss: 5.258  |  Val Loss: 5.421\n",
            "I went out with Lady Elizabeth to the meadows, I have all married upon itship,, necessary, for glad companion, she would made related in the contrary in spite at your were has sober his profession\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise: Play around with the model\n",
        "\n",
        "# experiment with prompts, try training the model with different configurations (be careful with CUDA out of memory issue!)\n",
        "# what is your impression? what do you notice?"
      ],
      "metadata": {
        "id": "X3ioHQop781n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 2**.&nbsp; Attention"
      ],
      "metadata": {
        "id": "pbq8Fez8pai_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention** is the main architecture component in transformer. It allows tokens to update their representation by learning from all other tokens in the sequence. This allows transformer lms to have nuanced, context dependent meaning of words and text sequences.\n",
        "\n",
        "To implement this, attention mechanism consists of three key elements for each token (token representation) in the sequence:\n",
        "\n",
        "- **Query**: Vector summarising which info the token is \"looking for\".\n",
        "- **Key**: Vector storing information that is used to \"index\" a token, to match with the query.\n",
        "- **Value**: Vector representing the actual \"content\" of token representations. Gets \"picked up\" by referening the relevant key for the given query.\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "\n",
        "Dot Product: mathematical operation that combines two vectors and yields a scalar value. Dot product is a measure of simlarity between vectors as it quantifies how closely two vectors are aligned (high - more aligned).\n",
        "\n"
      ],
      "metadata": {
        "id": "4pcZkTlNeiGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "EnK4Q7kadQ3z"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# create toy sequence of data\n",
        "x = torch.randn((2, 6, 10)) # 5 words by 10 emb dimension\n",
        "B, T, C = x.shape # input shape dims\n",
        "head_size = 5 # att dimension\n",
        "# all tokens to receive information from each other\n",
        "Q = torch.nn.Linear(C, head_size)\n",
        "K = torch.nn.Linear(C, head_size)\n",
        "V = torch.nn.Linear(C, head_size)\n",
        "\n",
        "Q = Q(x) # (T, C) @ (C, head_size) -> (T, head_size)\n",
        "K = K(x)\n",
        "V = V(x)\n",
        "\n",
        "QK = (Q @ K.transpose(-2, -1)) / torch.sqrt(torch.tensor(head_size))\n",
        "att = torch.softmax(QK, dim=-1)\n",
        "att = att @ V # (n tokens, head_size)"
      ],
      "metadata": {
        "id": "LY_ioCJQHgrm"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d9Ncizo1P4",
        "outputId": "51d94751-c238-46ac-f00e-ba2253be8e91"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, n_embd, head_size, contex_len):\n",
        "        \"\"\"\n",
        "        Single-head self-attention\n",
        "        n_embd : embedding dimension (i.e. the input feature size)\n",
        "        head_size : dimension for this particular head\n",
        "        contex_len : maximum sequence length (for constructing the causal mask)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "\n",
        "        self.Q = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.K = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.V = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # mask to hide \"future\" tokens (we only attend to current and previous tokens)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(contex_len, contex_len)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        q = self.Q(x) # (B, T, C) @ (C, head_size) -> (B, T, head_size)\n",
        "        k = self.K(x)\n",
        "        v = self.V(x)\n",
        "\n",
        "        # compute attention scores:\n",
        "        # shape: (B, T, head_size) @ (B, T, head_size).T -> (B, T, T)\n",
        "        # but we need the last dimension to match so we do a transpose on K:\n",
        "        att_weight = q @ k.transpose(-2, -1) # (B, T, T)\n",
        "\n",
        "        # mask\n",
        "        att_weight = att_weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # scale\n",
        "        att_weight = att_weight / k.shape[-1]**0.5\n",
        "\n",
        "        # normalize over last dimension\n",
        "        att_weight = torch.softmax(att_weight, dim=-1)\n",
        "\n",
        "\n",
        "        # weighted sum over V\n",
        "        att_weight = att_weight @ v\n",
        "        return att_weight"
      ],
      "metadata": {
        "id": "UMIlbHoRMaWT"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 3**.&nbsp; **Multi-Head Attention**\n",
        "\n"
      ],
      "metadata": {
        "id": "VqfkHJ742K9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words can have multiple different ways they relate to each other in text. They can convey grammatic relationships, different facets of meaning. For that reason, in transformer we implement not just one single attention mehanism to create represenatation of a text sequence, instead, we do multiple attention mechanisms (heads), to allow for different relationships at the same time.\n",
        "\n",
        "In implementation, we just split the attention stage into multiple chunks that run in parallel and independently, and then concatenate their results to get the final (for that stage) representation of the sequence."
      ],
      "metadata": {
        "id": "GiUo1CvU2O9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_embd, n_heads, context_len):\n",
        "        \"\"\"\n",
        "        Multi-head self-attention\n",
        "        n_embd : total embedding dimension\n",
        "        n_heads : how many separate attention heads\n",
        "        context_len : for the causal mask\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_heads\n",
        "\n",
        "        self.heads = torch.nn.ModuleList(\n",
        "            [SelfAttention(n_embd, head_size, context_len)\n",
        "            for _ in range(n_heads)]\n",
        "        )\n",
        "\n",
        "        self.proj = torch.nn.Linear(n_heads * head_size, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NUgfWMIS2OMM"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Remove RNN and Set MHA\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_embd,\n",
        "            tokenizer,\n",
        "            device=\"cpu\",\n",
        "            context_len=64,    # maximum sequence length\n",
        "            n_heads=4        # number of attention heads\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.context_len = context_len\n",
        "        self.vocab_size = tokenizer.n_vocab\n",
        "\n",
        "        self.embd = torch.nn.Embedding(self.vocab_size, n_embd)\n",
        "        self.pos_embd = torch.nn.Embedding(self.context_len, n_embd)\n",
        "        self.attn = MultiHeadAttention(n_embd, n_heads, context_len)\n",
        "        self.out = torch.nn.Linear(n_embd, self.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        embds = self.embd(x)  # (batch, context_len, n_embd)\n",
        "        # add positional embd\n",
        "        pos = torch.arange(0, seq_len, dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "        embds = embds + self.pos_embd(pos) # (B, seq_len, n_embd)\n",
        "        attention_out = self.attn(embds)  # (batch, context_len, n_embd)\n",
        "        logits = self.out(attention_out)  # (batch, context_len, vocab_size)\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=5, lr=1e-3):\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device)\n",
        "\n",
        "                logits = self(X).view(-1, self.vocab_size)\n",
        "                Y = Y.view(-1)\n",
        "                loss = loss_fn(logits, Y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            if val_loader:\n",
        "                self.eval()\n",
        "                total_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device)\n",
        "                        val_logits = self(Xv).view(-1, self.vocab_size)\n",
        "                        Yv = Yv.view(-1)\n",
        "                        val_loss = loss_fn(val_logits, Yv)\n",
        "                        total_val_loss += val_loss.item()\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_train_loss:.3f} | Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_train_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=16):\n",
        "        self.eval()\n",
        "        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # only consider last context_len tokens to manage memory usage\n",
        "            input_ids_cond = input_ids[:, -self.context_len:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self(input_ids_cond)\n",
        "\n",
        "            last_logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(last_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat((input_ids, next_token), dim=1)\n",
        "\n",
        "        return self.tokenizer.decode(input_ids[0].tolist())\n"
      ],
      "metadata": {
        "id": "8cJgyds-8YrP"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs about 20 seconds on a T4 GPU\n",
        "\n",
        "config = {\n",
        "    \"n_embd\": 64,\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\",\n",
        "    \"context_len\": 64,\n",
        "    \"n_heads\": 4\n",
        "}\n",
        "\n",
        "model = LanguageModel(**config)\n",
        "model.fit(train_loader, val_loader, epochs=5, lr=1e-3)\n",
        "\n",
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia7l-mIQTLMt",
        "outputId": "f6548e98-df18-4c28-d641-3400bac0884c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]  Train Loss: 8.853 | Val Loss: 6.706\n",
            "Epoch [2/5]  Train Loss: 6.423 | Val Loss: 6.468\n",
            "Epoch [3/5]  Train Loss: 6.295 | Val Loss: 6.395\n",
            "Epoch [4/5]  Train Loss: 6.193 | Val Loss: 6.278\n",
            "Epoch [5/5]  Train Loss: 6.014 | Val Loss: 6.096\n",
            "I went out with Lady Elizabeth to the meadows mere notvern\n",
            "againconfidence again char alarming instantly time from Endurance Mr of secured\n",
            "b close came young had was press thought her gladkeepingThings. than Crawford\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"It was a sunny day, I went out with Lady Margrett to the gardens.\"\n",
        "\n",
        "print(model.generate(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYLv5MruT2ZM",
        "outputId": "7e64a7cb-6253-4bc4-fd35-c8f5ca0216d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a sunny day, I went out with Lady Margrett to the gardens. He you be that it as I really owe had not staying some comfort, it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise: Play around with the model\n",
        "\n",
        "# experiment with prompts, try training the model with different configurations\n",
        "# what is your impression? what do you notice?"
      ],
      "metadata": {
        "id": "TIz8KBaOUMXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 4**.&nbsp; **Feed Froward Layer, Skip Connection and Layer Normalisation**"
      ],
      "metadata": {
        "id": "oO_U3MQTkeCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [**Feed Forward**]: Computation designed for inputs in our sequence to process the information they learned over all other inputs during multi-head attention step.  \n",
        "\n",
        "- [**Layer Normalisation**](https://arxiv.org/abs/1607.06450): When training deep neural networks (that is, networks with many layers), we can experience instability in gradient updating such as vanishing or exploding gradients. LayerNorm helps prevent this problem by rescaling the outputs of a nn.Layer to have mean of 0 and varience of 1. This adjustment speeds up the convergence to good weights and ensures consistent training.\n",
        "\n",
        "- [**Skip Connection**](https://arxiv.org/abs/1512.03385): Helps prevent vanishing gradients.\n",
        "\n",
        "- [**Dropout**](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf): Helps prevent overfitting.\n",
        "\n",
        "[Source: \"Build a Large Language Model from Scratch\" by Sebastian Raschka, ch 4.](https://www.manning.com/books/build-a-large-language-model-from-scratch?a_aid=raschka&a_bid=4c2437a0&chan=mm_github)"
      ],
      "metadata": {
        "id": "54_Ou4csW258"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FeedForward and Transformer Block Classes\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_embd, n_embd * 4),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(n_embd * 4, n_embd),\n",
        "            torch.nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)\n",
        "\n",
        "class TfBlock(torch.nn.Module):\n",
        "    def __init__(self, n_embd, n_heads, context_len, dropout):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_embd, n_heads, context_len, dropout)\n",
        "        self.ff = FeedForward(n_embd, dropout)\n",
        "        self.norm_1 = torch.nn.LayerNorm(n_embd)\n",
        "        self.norm_2 = torch.nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.norm_1(x)\n",
        "        x = x + self.mha(x)\n",
        "        x = x + self.norm_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KpgdrQK1w0i_",
        "cellView": "form"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add Dropout to Attention\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, n_embd, head_size, contex_len, dropout):\n",
        "        \"\"\"\n",
        "        Single-head self-attention\n",
        "        n_embd : embedding dimension (i.e. the input feature size)\n",
        "        head_size : dimension for this particular head\n",
        "        contex_len : maximum sequence length (for constructing the causal mask)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "\n",
        "        self.Q = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.K = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.V = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # mask to hide \"future\" tokens (we only attend to current and previous tokens)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(contex_len, contex_len)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        q = self.Q(x) # (B, T, C) @ (C, head_size) -> (B, T, head_size)\n",
        "        k = self.K(x)\n",
        "        v = self.V(x)\n",
        "\n",
        "        # compute attention scores:\n",
        "        # shape: (B, T, head_size) @ (B, T, head_size).T -> (B, T, T)\n",
        "        # but we need the last dimension to match so we do a transpose on K:\n",
        "        att_weight = q @ k.transpose(-2, -1) # (B, T, T)\n",
        "\n",
        "        # mask\n",
        "        att_weight = att_weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # scale\n",
        "        att_weight = att_weight / k.shape[-1]**0.5\n",
        "\n",
        "        # normalize over last dimension\n",
        "        att_weight = torch.softmax(att_weight, dim=-1)\n",
        "\n",
        "        # NEW: add dropout to prevent overfitting\n",
        "        att_weight = self.dropout(att_weight)\n",
        "\n",
        "        # weighted sum over V\n",
        "        att_weight = att_weight @ v\n",
        "        return att_weight\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, context_len, dropout):\n",
        "        \"\"\"\n",
        "        Multi-head self-attention\n",
        "        n_embd : total embedding dimension\n",
        "        num_heads : number of parallel attention heads\n",
        "        context_len : for the causal mask\n",
        "        dropout : probability for dropout\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        head_size = n_embd // num_heads  # dimension per head\n",
        "\n",
        "        self.heads = torch.nn.ModuleList([\n",
        "            SelfAttention(n_embd, head_size, context_len, dropout=dropout)\n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "        self.proj = torch.nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, sequence_length, n_embd)\n",
        "        Returns: (batch_size, sequence_length, n_embd)\n",
        "        \"\"\"\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # (B, context_len, num_heads * head_size)\n",
        "        out = self.proj(out)  # (B, T, n_embd)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "MUvowgAv3mnf",
        "cellView": "form"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Full Transformer Model\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_embd,\n",
        "        tokenizer,\n",
        "        device=\"cpu\",\n",
        "        dropout=0.2,\n",
        "        context_len=64,\n",
        "        n_heads=4,\n",
        "        n_blocks=3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.dropout = dropout\n",
        "        self.vocab_size = tokenizer.n_vocab\n",
        "        self.context_len = context_len\n",
        "\n",
        "        self.embd = torch.nn.Embedding(self.vocab_size, n_embd)\n",
        "        self.pos_embd = torch.nn.Embedding(self.context_len, n_embd)\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "            TfBlock(n_embd, n_heads, context_len, dropout)\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "        self.norm_final = torch.nn.LayerNorm(n_embd)\n",
        "        self.out = torch.nn.Linear(n_embd, self.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        embds = self.embd(x)  # (batch, context_len, n_embd)\n",
        "        # add positional embd\n",
        "        pos = torch.arange(0, seq_len, dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "        embds = embds + self.pos_embd(pos) # (B, seq_len, n_embd)\n",
        "        blocks_out = self.blocks(embds)                # (B, context_len, n_embd)\n",
        "        normed_out = self.norm_final(blocks_out)       # (B, context_len, n_embd)\n",
        "        logits = self.out(normed_out)                  # (B, context_len, vocab_size)\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=5, lr=1e-3):\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device)\n",
        "\n",
        "                logits = self(X).view(-1, self.vocab_size)\n",
        "                Y = Y.view(-1)\n",
        "                loss = loss_fn(logits, Y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            if val_loader is not None:\n",
        "                self.eval()\n",
        "                total_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device)\n",
        "                        val_logits = self(Xv).view(-1, self.vocab_size)\n",
        "                        Yv = Yv.view(-1)\n",
        "\n",
        "                        val_loss = loss_fn(val_logits, Yv)\n",
        "                        total_val_loss += val_loss.item()\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {avg_train_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {avg_train_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=16):\n",
        "        self.eval()\n",
        "        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            input_ids_cond = input_ids[:, -self.context_len:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self(input_ids_cond)\n",
        "\n",
        "            last_logits = logits[:, -1, :]\n",
        "            probs = F.softmax(last_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            input_ids = torch.cat((input_ids, next_token), dim=1)\n",
        "\n",
        "        return self.tokenizer.decode(input_ids[0].tolist())"
      ],
      "metadata": {
        "id": "PYK-QkLsyylE"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs for about 2 mins on T4\n",
        "\n",
        "config = {\n",
        "    \"n_embd\": 64,\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\",\n",
        "    \"context_len\": 64,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_blocks\": 2\n",
        "}\n",
        "\n",
        "model = LanguageModel(**config)\n",
        "model.fit(train_loader, val_loader, epochs=5, lr=1e-3)\n",
        "\n",
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wllHUbyu2a1V",
        "outputId": "48d47a22-0c93-404d-e36b-0201c29dba64"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]  Train Loss: 9.076  |  Val Loss: 6.826\n",
            "Epoch [2/5]  Train Loss: 6.482  |  Val Loss: 6.449\n",
            "Epoch [3/5]  Train Loss: 6.330  |  Val Loss: 6.407\n",
            "Epoch [4/5]  Train Loss: 6.256  |  Val Loss: 6.289\n",
            "Epoch [5/5]  Train Loss: 6.092  |  Val Loss: 6.118\n",
            "I went out with Lady Elizabeth to the meadowsious\n",
            " goodt dearony a another ever good and\n",
            "that! took to byÔøΩ farewell F or, only a without what any and,that, business\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4**.&nbsp; **Inference: Decoding Strategies**"
      ],
      "metadata": {
        "id": "SCEfWNnUpoZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding strategies define how a language model selects the next token when generating text.\n",
        "\n",
        "- **Greedy decoding**: model always picks the token with the highest probability at each step. While efficient, greedy decoding often leads to repetitive and uncreative outputs because it lacks diversity.\n",
        "- **Beam Search**: improves upon this by keeping multiple hypotheses (beams) at each step, selecting the best overall sequence rather than just the best token at each step.\n",
        "- **Temperature Scaling**: controls randomness by adjusting the probability distribution‚Äîhigher temperatures encourage more randomness, while lower temperatures make predictions more deterministic.\n",
        "- **Top K sampling**: ntroduces randomness by restricting token selection to only the top K most probable words at each step, effectively eliminating unlikely choices while maintaining diversity.\n",
        "\n"
      ],
      "metadata": {
        "id": "TANrszgUwGpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Very Final Model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_embd,\n",
        "        tokenizer,\n",
        "        device=\"cpu\",\n",
        "        dropout=0.2,\n",
        "        context_len=64,\n",
        "        n_heads=4,\n",
        "        n_blocks=3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.dropout = dropout\n",
        "        self.vocab_size = tokenizer.n_vocab\n",
        "        self.context_len = context_len\n",
        "\n",
        "        self.embd = torch.nn.Embedding(self.vocab_size, n_embd)\n",
        "        self.pos_embd = torch.nn.Embedding(self.context_len, n_embd)\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "            TfBlock(n_embd, n_heads, context_len, dropout)\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "        self.norm_final = torch.nn.LayerNorm(n_embd)\n",
        "        self.out = torch.nn.Linear(n_embd, self.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        embds = self.embd(x)\n",
        "        pos = torch.arange(seq_len, device=self.device).unsqueeze(0)\n",
        "        embds = embds + self.pos_embd(pos)\n",
        "        blocks_out = self.blocks(embds)\n",
        "        normed_out = self.norm_final(blocks_out)\n",
        "        logits = self.out(normed_out)\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=5, lr=1e-3):\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device)\n",
        "                logits = self(X).view(-1, self.vocab_size)\n",
        "                Y = Y.view(-1)\n",
        "                loss = loss_fn(logits, Y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            if val_loader is not None:\n",
        "                self.eval()\n",
        "                total_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device)\n",
        "                        val_logits = self(Xv).view(-1, self.vocab_size)\n",
        "                        Yv = Yv.view(-1)\n",
        "                        val_loss = loss_fn(val_logits, Yv)\n",
        "                        total_val_loss += val_loss.item()\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {avg_train_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {avg_train_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=16, temperature=1.0, top_k=50):\n",
        "        \"\"\"\n",
        "        Generate text from a prompt with temperature and top-k sampling.\n",
        "\n",
        "        Parameters:\n",
        "        - prompt (str): The input text to generate from.\n",
        "        - max_new_tokens (int): Number of tokens to generate.\n",
        "        - temperature (float): Higher values increase randomness, lower values make generation more deterministic.\n",
        "        - top_k (int): Limit token selection to the top K most probable tokens.\n",
        "\n",
        "        Returns:\n",
        "        - str: The generated text.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            input_ids_cond = input_ids[:, -self.context_len:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self(input_ids_cond)\n",
        "                logits = logits[:, -1, :]  # Get logits for last token only\n",
        "\n",
        "                # Temperature scaling\n",
        "                logits /= max(temperature, 1e-5)\n",
        "\n",
        "                # Top-k filtering\n",
        "                if top_k > 0:\n",
        "                    top_k_values, _ = torch.topk(logits, k=top_k)\n",
        "                    min_top_k = top_k_values[:, -1].unsqueeze(-1)\n",
        "                    logits = torch.where(\n",
        "                        logits < min_top_k,\n",
        "                        torch.tensor(float('-inf'), device=self.device),\n",
        "                        logits\n",
        "                    )\n",
        "\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        return self.tokenizer.decode(input_ids[0].tolist())"
      ],
      "metadata": {
        "id": "pIN9FpR2cR7C",
        "cellView": "form"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes up to 3 mins on T4 for 15 epchs\n",
        "config = {\n",
        "    \"n_embd\": 128,\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\",\n",
        "    \"context_len\": 128,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_blocks\": 2\n",
        "}\n",
        "\n",
        "\n",
        "model = LanguageModel(**config)\n",
        "model.fit(train_loader, val_loader, epochs=15, lr=1e-3)\n",
        "\n",
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=128))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcVqoMJ1dx5-",
        "outputId": "d638d9d3-a0b1-4ac8-acd6-2505c7f5594a"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]  Train Loss: 8.043  |  Val Loss: 6.511\n",
            "Epoch [2/15]  Train Loss: 6.342  |  Val Loss: 6.359\n",
            "Epoch [3/15]  Train Loss: 6.143  |  Val Loss: 6.099\n",
            "Epoch [4/15]  Train Loss: 5.843  |  Val Loss: 5.844\n",
            "Epoch [5/15]  Train Loss: 5.596  |  Val Loss: 5.695\n",
            "Epoch [6/15]  Train Loss: 5.386  |  Val Loss: 5.568\n",
            "Epoch [7/15]  Train Loss: 5.190  |  Val Loss: 5.459\n",
            "Epoch [8/15]  Train Loss: 5.008  |  Val Loss: 5.368\n",
            "Epoch [9/15]  Train Loss: 4.842  |  Val Loss: 5.295\n",
            "Epoch [10/15]  Train Loss: 4.689  |  Val Loss: 5.242\n",
            "Epoch [11/15]  Train Loss: 4.549  |  Val Loss: 5.203\n",
            "Epoch [12/15]  Train Loss: 4.426  |  Val Loss: 5.168\n",
            "Epoch [13/15]  Train Loss: 4.312  |  Val Loss: 5.156\n",
            "Epoch [14/15]  Train Loss: 4.210  |  Val Loss: 5.142\n",
            "Epoch [15/15]  Train Loss: 4.116  |  Val Loss: 5.137\n",
            "I went out with Lady Elizabeth to the meadows always,\n",
            "her_.lect, which any she did_ for the park and there is it.\n",
            "‚Äù And she had never do not; but this idea for everybody of her\n",
            "\n",
            "that some time to her spirits to her hand.\n",
            "\n",
            "‚Äôs\n",
            "Edmund: to the othersacy was ‚ÄúI dare say as,‚Äôs ass for my word very impossibility that areions,\n",
            "that very great institution the stationary\n",
            "And that was then\n",
            "dressed to feel, and would never have heard as she never seen good, if Miss Crawford‚Äôs eyes\n",
            "‚Äôtrying was quite as if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"It was a sunny day, I went out with Lady Margrett to the gardens.\"\n",
        "for i in range(10):\n",
        "    print(model.generate(prompt, max_new_tokens=32, temperature=1.5), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0ksCt--gNJD",
        "outputId": "5c10154d-2554-4154-a46c-1915ea79ec15"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a sunny day, I went out with Lady Margrett to the gardens. Five on him so many\n",
            "c c us as I think in such a most\n",
            "whiched by her sisteringness about Fanny: the Admiral\n",
            "was \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. You as he only\n",
            "conferred first that of the shade: so very difficult, were the whole in future, was only two; her first three boys\n",
            " \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. To the\n",
            "not sympath\n",
            "nothinged about the heat. Grant was always answered, that how. My time of it had never and then out to her\n",
            " \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. How, from the same Park, they; she would know\n",
            "not be an answer to appear in a point of an early and langu said her good. You \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. Lady Bertram\n",
            "was enduring, her to be done into what you them the world on her own family the morrow, I hope Mr, the most at \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. Poor, for\n",
            "conce from Mansfield, were now it to-sized family is to a point of restraint that. All his sister‚Äôs account \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. Do not to say you shall certainly it.‚Äù\n",
            "\n",
            "‚ÄúAh that they did_. It is to her mother‚Äù\n",
            "\n",
            "‚Äú \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. It became her\n",
            "little\n",
            "very close to find\n",
            "f have seen the room as it would not understand, the house was a\n",
            "occ a friend up and \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. We I must have so much\n",
            "cr, the gardener of all his mother it\n",
            "could be made and with little by him, therefore, he were the \n",
            "\n",
            "It was a sunny day, I went out with Lady Margrett to the gardens. Grant I\n",
            "but you, that will not\n",
            "such a year of that you will to go. And her husband to the party me as he loves all the \n",
            "\n"
          ]
        }
      ]
    }
  ]
}